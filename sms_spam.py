# -*- coding: utf-8 -*-
"""sms_spam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ob0zijggK9H-1aAoAXK6YXjI2tMcWjwW

Berikut adalah multiklasifikasi untuk kasus **SMS**. <br> 
Disini akan dikelompokan kategori sms yang mengandung konten sms normal (sms), sms penipuan(penipuan), dan sms promo(promo).
"""

import csv
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional, Masking

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

df_train = pd.read_csv('/content/drive/My Drive/spam.csv')

df_train

"""**Data preprocessing and Cleaning**"""

category = pd.get_dummies(df_train.label)

print(category)

"""**Terdapat 1000 Sample**"""

df_baru = pd.concat([df_train, category], axis=1)
df_baru = df_baru.drop(columns='label')
df_baru

teks = df_baru['Teks'].values
label = df_baru[['penipuan', 'promo', 'sms']].values

"""**Training and Validation Spliting** <br>
dengan 20% data test. sudah menggunakan fungsi tokenizer untuk menggabungkan data teks.
"""

from sklearn.model_selection import train_test_split
teks_latih, teks_test, label_latih, label_test = train_test_split(teks, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(teks_latih) 
tokenizer.fit_on_texts(teks_test)
 
sekuens_latih = tokenizer.texts_to_sequences(teks_latih)
sekuens_test = tokenizer.texts_to_sequences(teks_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

"""Terdapat 3 kelas kategorikal, dilihat dari shape."""

print(label.shape)

"""**Pembuatan Model Layer** <br>
menggunakan embedding dan LSTM. relu dibuat agar fully connected antar layer
sementara dropout untuk mengurangi overfitting.
"""

model=Sequential()

model.add(Embedding(vocab_size,16))
model.add(Masking(mask_value=0.0))
# model.add(Dropout(0.6))
model.add((LSTM(16)))
model.add(Dense(8, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(3,activation='softmax'))
model.summary()

from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger
modelcheckpoint = ModelCheckpoint(
    filepath='./nlpsms.hdf5',
    monitor='loss',
    save_best_only=True,
    period=1
)
earlystopping = EarlyStopping(
    monitor='loss', 
    min_delta=0.001, 
    patience=10
)

"""**Compile Model**"""

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'],
)

"""**Model Fitting dan Proses Training**"""

num_epochs = 100
history = model.fit(padded_latih, label_latih, epochs=num_epochs, validation_data=(padded_test, label_test), verbose=2,callbacks=[modelcheckpoint, earlystopping])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Testing** <br>
testing dilakukan dengan tidak menggunakan data selain data training
mesin sudah mampu memprediksi label yang dimaksud.
"""

txt = ["dapatkan hadiah menarik dari kami"]

seq = tokenizer.texts_to_sequences(txt)
padded = pad_sequences(seq, maxlen=max_length)
pred = model.predict(padded)
labels = ['penipuan', 'promo', 'sms']

print(pred)
print(np.argmax(pred))
print(labels[np.argmax(pred)])

txt = ["ayo download penawaran kami"]

seq = tokenizer.texts_to_sequences(txt)
padded = pad_sequences(seq, maxlen=max_length)
pred = model.predict(padded)
labels = ['penipuan', 'promo', 'sms']

print(pred)
print(np.argmax(pred))
print(labels[np.argmax(pred)])

txt = ["Waalaikumsalamin apa yg dpt saya bantu min?"]

seq = tokenizer.texts_to_sequences(txt)
padded = pad_sequences(seq, maxlen=max_length)
pred = model.predict(padded)
labels = ['penipuan', 'promo', 'sms']

print(pred)
print(np.argmax(pred))
print(labels[np.argmax(pred)])

txt = ["pulsamu tinggal dikit? Beli pakai pulsa aja Bro, dapatkan sekarang "]
seq = tokenizer.texts_to_sequences(txt)
padded = pad_sequences(seq, maxlen=max_length)
pred = model.predict(padded)
labels = ['penipuan', 'promo', 'sms']

print(pred)
print(np.argmax(pred))
print(labels[np.argmax(pred)])

txt = ["lagi apa sayang?"]
seq = tokenizer.texts_to_sequences(txt)
padded = pad_sequences(seq, maxlen=max_length)
pred = model.predict(padded)
labels = ['penipuan', 'promo', 'sms']

print(pred)
print(np.argmax(pred))
print(labels[np.argmax(pred)])

"""-----------------------------------------------------------
**Profil Peserta**

Nama Lengkap : Ilham Winar Nugroho <br>
Nomer Telepon : 087838014299 <br>
Domisili : Tangerang <br>
Tentang Saya : Tertarik Belajar Machine Learning <br>
Pekerjaan : Pegawai BUMN <br>
"""